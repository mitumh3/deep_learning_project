{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prov-GigaPath Demo\n",
    "\n",
    "This notebook provides a quick walkthrough of the Prov-GigaPath models. We will start by demonstrating how to download the Prov-GigaPath models from HuggingFace. Next, we will show an example of pre-processing a slide. Finally, we will demonstrate how to run Prov-GigaPath on the sample slide.\n",
    "\n",
    "### Prepare HF Token\n",
    "\n",
    "To begin, please request access to the model from our HuggingFace repository: https://huggingface.co/prov-gigapath/prov-gigapath.\n",
    "\n",
    "Once approved, set the HF_TOKEN to access the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please set your Hugging Face API token\n",
    "os.environ[\"HF_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide_path = \"TCGA-YB-A89D-01A-01-TS1.53D8C086-00CC-4565-8C52-E81E8F8A8409.svs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "error due to import openslide: conda install -c conda-forge openslide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing slide TCGA-YB-A89D-01A-01-TS1.53D8C086-00CC-4565-8C52-E81E8F8A8409.svs at level 1 with tile size 256. Saving to outputs/preprocessing.\n",
      "('slide_id', 'tile_id', 'image', 'label', 'tile_x', 'tile_y', 'occupancy')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/gigapath/lib/python3.9/site-packages/numpy/core/_methods.py:118: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "Tiles (TCGA-Y…): 100%|██████████| 674/674 [00:08<00:00, 81.08img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slide TCGA-YB-A89D-01A-01-TS1.53D8C086-00CC-4565-8C52-E81E8F8A8409.svs has been tiled. 674 tiles saved to outputs/preprocessing/output/TCGA-YB-A89D-01A-01-TS1.53D8C086-00CC-4565-8C52-E81E8F8A8409.svs.\n"
     ]
    }
   ],
   "source": [
    "from gigapath.pipeline import tile_one_slide\n",
    "\n",
    "tmp_dir = 'outputs/preprocessing/'\n",
    "tile_one_slide(slide_path, save_dir=tmp_dir, level=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Load the tile images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 674 image tiles\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# load image tiles\n",
    "slide_dir = \"outputs/preprocessing/output/\" + os.path.basename(slide_path) + \"/\"\n",
    "image_paths = [os.path.join(slide_dir, img) for img in os.listdir(slide_dir) if img.endswith('.png')]\n",
    "\n",
    "print(f\"Found {len(image_paths)} image tiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Load the Prov-GigaPath model (tile and slide encoder models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device (Apple Silicon)\n",
      "Loading tile encoder from local path: ./local_tile_encoder.pth\n",
      "Tile encoder param # 1134953984\n",
      "Loading slide encoder from local path: ./local_slide_encoder.pth\n",
      "Slide encoder param # 86330880\n"
     ]
    }
   ],
   "source": [
    "from gigapath.pipeline import load_tile_slide_encoder\n",
    "\n",
    "# Load the tile and slide encoder models\n",
    "# NOTE: The CLS token is not trained during the slide-level pretraining.\n",
    "# Here, we enable the use of global pooling for the output embeddings.\n",
    "tile_encoder, slide_encoder_model = load_tile_slide_encoder(global_pool=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Run tile-level inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device (Apple Silicon)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference with tile encoder: 100%|██████████| 6/6 [07:29<00:00, 75.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tile_encoder_outputs[tile_embeds].shape: torch.Size([674, 1536])\n",
      "tile_encoder_outputs[coords].shape: torch.Size([674, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from gigapath.pipeline import run_inference_with_tile_encoder\n",
    "\n",
    "tile_encoder_outputs = run_inference_with_tile_encoder(image_paths, tile_encoder)\n",
    "\n",
    "for k in tile_encoder_outputs.keys():\n",
    "    print(f\"tile_encoder_outputs[{k}].shape: {tile_encoder_outputs[k].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tile_embeds': tensor([[-2.9516,  0.0538, -0.4163,  ..., -1.5307, -1.4200,  0.5797],\n",
       "         [-0.1761, -0.6626,  0.8080,  ..., -1.7176,  0.4222,  1.4378],\n",
       "         [-2.4339,  1.0733,  0.0420,  ..., -0.8383,  0.0587,  0.2867],\n",
       "         ...,\n",
       "         [-1.0461, -1.0824, -0.3474,  ..., -1.7699, -1.1944,  1.4791],\n",
       "         [ 0.7696, -0.1630, -0.0108,  ...,  0.3171, -0.7651,  0.3911],\n",
       "         [-1.6933, -0.7776,  0.4649,  ..., -0.7656,  0.3830,  0.5618]]),\n",
       " 'coords': tensor([[ 5692., 18568.],\n",
       "         [78398.,  2184.],\n",
       "         [17980., 14472.],\n",
       "         ...,\n",
       "         [86590.,  9352.],\n",
       "         [80446.,  3208.],\n",
       "         [26172., 13448.]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tile_encoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Run slide-level inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gigapath.pipeline import run_inference_with_slide_encoder\n",
    "\n",
    "# run inference with the slide encoder\n",
    "slide_embeds = run_inference_with_slide_encoder(slide_encoder_model=slide_encoder_model, **tile_encoder_outputs)\n",
    "print(slide_embeds.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
